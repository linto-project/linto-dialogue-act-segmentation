{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel LinTo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "#### 1. Data loading\n",
    "#### 2. Label definition\n",
    "#### 3. Train/test/dev sets separation\n",
    "#### 4. Rules writing\n",
    "#### 5. Application of the rules\n",
    "#### 6. Generative model\n",
    "#### 7. Analysis of the results\n",
    "#### 8. Useful files creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Turn off TensorFlow logging messages\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# For reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "DISPLAY_ALL_TEXT = True\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0 if DISPLAY_ALL_TEXT else 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# if not loaded : french model\n",
    "#! python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>beg_word</th>\n",
       "      <th>end_word</th>\n",
       "      <th>n_turn</th>\n",
       "      <th>beg_turn</th>\n",
       "      <th>end_turn</th>\n",
       "      <th>rank_turn</th>\n",
       "      <th>text_turn</th>\n",
       "      <th>sil_bef</th>\n",
       "      <th>sil_aft</th>\n",
       "      <th>...</th>\n",
       "      <th>token_bef</th>\n",
       "      <th>token_aft</th>\n",
       "      <th>dep</th>\n",
       "      <th>dep_bef</th>\n",
       "      <th>dep_aft</th>\n",
       "      <th>headpos</th>\n",
       "      <th>headpos_bef</th>\n",
       "      <th>headpos_aft</th>\n",
       "      <th>energy_aft</th>\n",
       "      <th>pitch_aft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ok</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.123</td>\n",
       "      <td>0</td>\n",
       "      <td>ok donc juste comme on enregistre euh donc là c' est le le deuxième scénario</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>advmod</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>donc</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.123</td>\n",
       "      <td>1</td>\n",
       "      <td>ok donc juste comme on enregistre euh donc là c' est le le deuxième scénario</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADV</td>\n",
       "      <td>advmod</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>advmod</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP</td>\n",
       "      <td>VERB</td>\n",
       "      <td>D</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>juste</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.123</td>\n",
       "      <td>2</td>\n",
       "      <td>ok donc juste comme on enregistre euh donc là c' est le le deuxième scénario</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>ADV</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>advmod</td>\n",
       "      <td>advmod</td>\n",
       "      <td>mark</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADP</td>\n",
       "      <td>VERB</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comme</td>\n",
       "      <td>3.92</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.123</td>\n",
       "      <td>3</td>\n",
       "      <td>ok donc juste comme on enregistre euh donc là c' est le le deuxième scénario</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>ADV</td>\n",
       "      <td>PRON</td>\n",
       "      <td>mark</td>\n",
       "      <td>advmod</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>U</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.123</td>\n",
       "      <td>4</td>\n",
       "      <td>ok donc juste comme on enregistre euh donc là c' est le le deuxième scénario</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>VERB</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>mark</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADP</td>\n",
       "      <td>D</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  beg_word  end_word  n_turn  beg_turn  end_turn  rank_turn  \\\n",
       "0  ok     2.00      2.37      0       0.0       10.123    0           \n",
       "1  donc   2.73      3.01      0       0.0       10.123    1           \n",
       "2  juste  3.38      3.67      0       0.0       10.123    2           \n",
       "3  comme  3.92      4.22      0       0.0       10.123    3           \n",
       "4  on     4.23      4.67      0       0.0       10.123    4           \n",
       "\n",
       "                                                                      text_turn  \\\n",
       "0  ok donc juste comme on enregistre euh donc là c' est le le deuxième scénario   \n",
       "1  ok donc juste comme on enregistre euh donc là c' est le le deuxième scénario   \n",
       "2  ok donc juste comme on enregistre euh donc là c' est le le deuxième scénario   \n",
       "3  ok donc juste comme on enregistre euh donc là c' est le le deuxième scénario   \n",
       "4  ok donc juste comme on enregistre euh donc là c' est le le deuxième scénario   \n",
       "\n",
       "   sil_bef  sil_aft  ...  token_bef  token_aft     dep  dep_bef  dep_aft  \\\n",
       "0  1.26     0.00     ...  ADP        ADV        ROOT    ROOT     advmod    \n",
       "1  0.00     0.24     ...  ADP        ADV        advmod  ROOT     advmod    \n",
       "2  0.24     0.00     ...  ADV        SCONJ      advmod  advmod   mark      \n",
       "3  0.00     0.00     ...  ADV        PRON       mark    advmod   nsubj     \n",
       "4  0.00     0.12     ...  SCONJ      VERB       nsubj   mark     xcomp     \n",
       "\n",
       "   headpos  headpos_bef  headpos_aft  energy_aft  pitch_aft  \n",
       "0  ADP      ADP          ADP          0           0          \n",
       "1  ADP      ADP          VERB         D           U          \n",
       "2  VERB     ADP          VERB         D           D          \n",
       "3  VERB     VERB         VERB         U           D          \n",
       "4  VERB     VERB         ADP          D           U          \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete dataframe loading with columns : \n",
    "#      'word', 'beg_word', 'end_word', 'n_round',\n",
    "#       'beg_round', 'end_round', 'rank_round', 'text_round', 'sil_bef',\n",
    "#       'sil_aft', 'tony', 'gold', 'prob_nothing', 'prob_point', 'prob_comma',\n",
    "#       'prob_nothing_bef', 'prob_point_bef', 'prob_comma_bef',\n",
    "#       'real_rank_round', 'pitch', 'energy', 'pitch_bef', 'energy_bef',\n",
    "#       'punct', 'punct_bef', 'file', 'tony_realturns', 'token', 'token_bef',\n",
    "#      'token_aft', 'dep', 'dep_bef', 'dep_aft', 'headpos', 'headpos_bef',\n",
    "#      'headpos_aft', 'energy_aft', 'pitch_aft'\n",
    "\n",
    "df_linto = pd.read_csv(\"../data/df_all_final_LinTo_17032021.csv\")\n",
    "df_linto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do : check if every columns are presents in the loaded file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Label definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = -1\n",
    "NO = 0\n",
    "BOS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/test/dev sets separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev length =  9752 words \n",
      "test length =  8833 words \n",
      "train length =  53692 words\n"
     ]
    }
   ],
   "source": [
    "df_dev = df_linto[(df_linto[\"file\"]==\"Linagora_R1\") | (df_linto[\"file\"]==\"Linagora_A1\")]\n",
    "df_test = df_linto[(df_linto[\"file\"]==\"Linagora_P1\") | (df_linto[\"file\"]==\"Linagora_C1\")]\n",
    "df_train=df_linto[(df_linto[\"file\"]!=\"Linagora_R1\")&(df_linto[\"file\"]!=\"Linagora_A1\")&(df_linto[\"file\"]!=\"Linagora_P1\")&(df_linto[\"file\"]!=\"Linagora_C1\")]\n",
    "\n",
    "\n",
    "Y_test = df_test[\"gold\"].values\n",
    "Y_dev = df_dev[\"gold\"].values\n",
    "\n",
    "print(\"dev length = \", len(df_dev),\"words \\ntest length = \", len(df_test),\"words \\ntrain length = \", len(df_train), \"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rules writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic rules (without preprocessor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "\n",
    "# If ToNy (written) predicts BOS -> BOS\n",
    "@labeling_function()\n",
    "def tony(x):\n",
    "    return BOS if x[\"tony\"]==1 else ABSTAIN\n",
    "\n",
    "#  If beginning of turn -> BOS \n",
    "@labeling_function()\n",
    "def beg_turn(x):\n",
    "    return BOS if x[\"rank_turn\"]==0 else ABSTAIN\n",
    "\n",
    "# If period likelihood just before the current word > 0.3 -> BOS \n",
    "@labeling_function()\n",
    "def period_bef(x):\n",
    "    return BOS if x[\"prob_period_bef\"]>0.3 and x[\"word\"]!=\"euh\" else ABSTAIN\n",
    "\n",
    "# If no punctuation likelihood after the current word > 0.99 -> NO\n",
    "@labeling_function()\n",
    "def nothing_bef(x):\n",
    "    return NO if x[\"prob_nothing_bef\"]>0.99 else ABSTAIN\n",
    "\n",
    "# If comma likelihood just before the current word > 0.3 -> BOS\n",
    "@labeling_function()\n",
    "def comma_bef(x):\n",
    "     return BOS if x[\"prob_comma_bef\"]>0.3 else ABSTAIN\n",
    "\n",
    "\n",
    "# If repetitions/hesitations (\"euh\" aside) -> NO\n",
    "@labeling_function()\n",
    "def no_disfluency(x):\n",
    "    param_context=4\n",
    "    list_text=str(x[\"text_turn\"]).split(\" \")\n",
    "    pos=int(x[\"rank_turn\"])\n",
    "    context=[]\n",
    "    cmpt=0\n",
    "    if pos>param_context and pos<len(list_text)-param_context:\n",
    "        for e in list_text[pos-param_context:pos+param_context]:\n",
    "            context.append(e)\n",
    "        while \"euh\" in context:\n",
    "            context.remove(\"euh\")\n",
    "            cmpt+=1\n",
    "        if abs(len(set(context))-len(context))>=2 or cmpt>2:\n",
    "            return NO\n",
    "        \n",
    "    param_context=2\n",
    "    context=[]\n",
    "    cmpt=0\n",
    "    if pos>param_context and pos<len(list_text)-param_context:\n",
    "        for e in list_text[pos-param_context:pos+param_context]:\n",
    "            context.append(e)\n",
    "        if len(set(context))!=len(context):\n",
    "            return NO\n",
    "    return ABSTAIN\n",
    "\n",
    "# If energy and pitch follow a particular pattern -> BOS\n",
    "@labeling_function()\n",
    "def energy_pitch(x):\n",
    "    liste_features=[\"DUUS\", \"DDUS\", \"DDUU\", \"SDUD\", \"SUUD\", \"SUUU\"]\n",
    "    str_feat=str(x.energy_bef)+str(x.energy)+str(x.pitch_bef)+str(x.pitch)\n",
    "    \n",
    "    if str_feat in liste_features and x.word!=\"euh\" and x.sil_bef>0.3:\n",
    "        return BOS\n",
    "\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def energy_pitch2(x):\n",
    "    if x.sil_bef>0.3:\n",
    "        if (x.energy_bef==\"D\" and x.pitch_bef==\"U\") and x.word!=\"euh\":\n",
    "            return BOS\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "\n",
    "# If word belongs to a specific type -> BOS \n",
    "@labeling_function()\n",
    "def cc(x):\n",
    "    notype=[\"ADJ\", \"DET\", \"AUX\", \"VERB\"]\n",
    "    if x.i_token in liste_begcc :#and x.token not in notype:\n",
    "        return BOS\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "\n",
    "liste_tok=[\"CCONJ\"]\n",
    "liste_tok_bef=[\"NOUN\"]\n",
    "liste_tok_dep=[\"nsubj\", \"cc\", \"advmod\", \"nmod\", \"amod\", \"expl:subj\", \"obj\", \"dep\"]\n",
    "\n",
    " \n",
    "@labeling_function()\n",
    "def pos(x):\n",
    "    if x.token in liste_tok and x.token_bef!=\"CCONJ\" and x.token_aft!=\"NOUN\" and x.prob_comma_bef>0.05:\n",
    "        return BOS\n",
    "    return ABSTAIN\n",
    "\n",
    "# If previous, current and next word belongs to specific types -> BOS \n",
    "@labeling_function()\n",
    "def cconj(x):\n",
    "    liste_no=[\"euh\", \"pas\", \"que\", \"ou\", \"tu\", \"aussi\", \"là\"]\n",
    "    if (x.token_bef==\"NOUN\" and  x.token==\"CCONJ\" and x.token_aft==\"PRON\") or (x.token_bef==\"NOUN\" and  x.token==\"SCONJ\" and x.token_aft==\"SCONJ\") or (x.token_bef==\"NOUN\" and  x.token==\"ADV\" and x.token_aft==\"PRON\") and x.word not in liste_no:\n",
    "        return BOS\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "# Si le modèle de samir met un period avant le mot -> BOS\n",
    "@labeling_function()\n",
    "def punct_samir_period(x):\n",
    "    if x.punct_bef==\".\" and x.word!=\"euh\" and x.sil_bef>0:\n",
    "        return BOS\n",
    "    return ABSTAIN \n",
    "\n",
    "# Si le modèle de samir met une virgule avant le mot -> BOS\n",
    "@labeling_function()\n",
    "def punct_samir_comma(x):\n",
    "    if x.punct_bef==\",\" and x.word!=\"euh\" and x.sil_bef>0:\n",
    "        return BOS\n",
    "    return ABSTAIN \n",
    "\n",
    "# Si le mot d'avant est un NOUN et l'énergie avant est descendante (D) -> BOS\n",
    "@labeling_function()\n",
    "def noun_d(x):\n",
    "    liste_tok=[\"PRON\", \"ADV\", \"CCONJ\"]\n",
    "    if x.token_bef==\"NOUN\" and x.energy_bef==\"D\" and x.token in liste_tok and x.energy!=\"D\":\n",
    "        return BOS\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "# Si début de vrai tour -> BOS\n",
    "@labeling_function()\n",
    "def beg_real_turn(x):\n",
    "    if x.real_rank_turn==0:\n",
    "        return BOS\n",
    "    return ABSTAIN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules using preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessor creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "# The SpacyPreprocessor parses the text in text_field and\n",
    "# stores the new enriched representation in doc_field\n",
    "spacy = SpacyPreprocessor(text_field=\"word\",language='fr_core_news_sm', doc_field=\"doc\", memoize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "# give access to the left and right word of the current word.\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def word_context_markers(x):\n",
    "    list_text=str(x[\"text_turn\"]).split(\" \")\n",
    "    pos=int(x[\"rank_turn\"])\n",
    "    if pos<len(list_text)-1 and pos>0:\n",
    "        x.word_bef = list_text[pos-1]\n",
    "        x.word_aft = list_text[pos+1]\n",
    "    elif pos==0 and pos<len(list_text)-1:\n",
    "        x.word_bef = np.nan\n",
    "        x.word_aft= list_text[pos+1]\n",
    "    elif pos==len(list_text)-1 and pos==0:\n",
    "        x.word_aft = np.nan\n",
    "        x.word_bef = list_text[pos-1]\n",
    "    else:\n",
    "        x.word_aft = np.nan\n",
    "        x.word_bef = np.nan\n",
    "    #return x\n",
    "\n",
    "\n",
    "# set the atrribut x.marker to 1 if the word is a isolated discursive marker, or the first of a list of markers\n",
    "    couples_markers=dict(du=[\"coup\"], de=[\"fait\"], en=[\"fait\", \"vrai\", \"effet\"], parce=[\"que\",\"qu'\"], et=[\"puis\", \"donc\"],enfin=[\"bon\"],mais=[\"bon\"], à=[\"propos\"], tu=[\"vois\",\"sais\"])\n",
    "    single_markers=[\"ouais\", \"donc\",\"effectivement\",\"alors\",\"bref\",\"voilà\", \"ensuite\", \"mais\", \"ben\", \"hein\", \"enfin\", \"finalement\", \"pourtant\", \"cependant\"]\n",
    "    \n",
    "    list_text=str(x[\"text_turn\"]).split(\" \")\n",
    "    wrd=str(x[\"word\"])\n",
    "    pos=int(x[\"rank_turn\"])\n",
    "    \n",
    "    m=0\n",
    "    if pos!=len(list_text)-1:\n",
    "        wrd_aft=str(list_text[pos+1])\n",
    "        if wrd in single_markers or ((wrd in couples_markers.keys()) and (wrd_aft in couples_markers[wrd])):\n",
    "            if pos==0:\n",
    "                m=1\n",
    "            else:\n",
    "                wrd_bef=str(list_text[pos-1])\n",
    "                if wrd_bef in single_markers:\n",
    "                    m=0\n",
    "                else:\n",
    "                    if pos==1:\n",
    "                        m=1\n",
    "                    else:\n",
    "                        wrd_bef_bef=str(list_text[pos-2])\n",
    "                        if wrd_bef_bef in couples_markers.keys() and wrd_bef in couples_markers[wrd_bef_bef]:\n",
    "                            m=0\n",
    "                        else:\n",
    "                            m=1\n",
    "    x.marker=m\n",
    "\n",
    "# set the attribut x.aft_mark to 1 if the word follows a serie of markers or one marker.\n",
    "\n",
    "    couples_markers=dict(du=[\"coup\"], de=[\"fait\"], en=[\"fait\", \"vrai\", \"effet\"], parce=[\"que\",\"qu'\"], et=[\"puis\", \"donc\"],enfin=[\"bon\"],mais=[\"bon\"], à=[\"propos\"], tu=[\"vois\",\"sais\"])\n",
    "    single_markers=[\"ouais\", \"donc\",\"effectivement\",\"alors\",\"bref\",\"voilà\", \"et\", \"ensuite\", \"mais\", \"ben\", \"hein\", \"enfin\", \"finalement\", \"pourtant\", \"cependant\"]\n",
    "    \n",
    "    list_text=str(x[\"text_turn\"]).split(\" \")\n",
    "    wrd=str(x[\"word\"])\n",
    "    pos=int(x[\"rank_turn\"])\n",
    "    bef=0\n",
    "    aft_mark=0\n",
    "    if pos>0:\n",
    "        wrd_bef=str(list_text[pos-1])\n",
    "        for k in couples_markers.keys():\n",
    "            if wrd_bef in couples_markers[k]:\n",
    "                bef=1\n",
    "        #print(wrd, wrd_bef, bef)\n",
    "        if wrd_bef in single_markers or bef==1:\n",
    "            if wrd not in single_markers and wrd not in couples_markers.keys():\n",
    "                aft_mark=1\n",
    "            else:\n",
    "                aft_mark=0\n",
    "        else:\n",
    "            aft_mark=0\n",
    "    \n",
    "    x.aft_mark=aft_mark\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processor : word_context_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If current word is an isolated marker or ther first of a serie -> BOS\n",
    "@labeling_function(pre=[word_context_markers])\n",
    "def first_marker(x):\n",
    "    if x.marker==1:\n",
    "        return BOS\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the current word follows an isolated marker or a serie of markers -> NO\n",
    "@labeling_function(pre=[word_context_markers])\n",
    "def no_after_markers(x):\n",
    "    if x.aft_mark==1:\n",
    "        return NO\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "    \n",
    "# If energy/f0 follow a specific pattern -> NO\n",
    "@labeling_function(pre=[word_context_markers])\n",
    "def no_energy_pitch(x):\n",
    "    list_nrj_bef=[\"S\", \"U\"]\n",
    "    list_pitch_bef=[\"D\", \"S\"]\n",
    "    beg_markers=[\"bonjour\", \"quand\", \"par\",\"merci\", \"ok\", \"bon\",\"ouais\",\"oui\",\"pour\", \"donc\",\"effectivement\",\"alors\",\"bref\",\"voilà\", \"et\", \"ensuite\", \"mais\", \"ben\", \"hein\", \"enfin\", \"finalement\", \"pourtant\", \"cependant\", \"parce\"]\n",
    "    pronoms=[\"je\", \"tu\", \"il\", \"vous\", \"on\", \"nous\", \"elle\", \"ils\",\"qui\", \"que\", \"j'\",\"c'\"]\n",
    "    if (x.energy_bef in list_nrj_bef and x.pitch_bef in list_pitch_bef)or  (x.energy_bef==\"D\" and x.energy==\"D\"):\n",
    "        if x.marker!=1 and x.word not in pronoms:\n",
    "            return NO\n",
    "    return ABSTAIN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processor : spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If word is a specific type -> NO\n",
    "@labeling_function(pre=[spacy])\n",
    "def no_type(x):\n",
    "    \"\"\"Ham comments mention specific people and are short.\"\"\"\n",
    "    if  any([(token.pos_ == \"ADJ\" or token.pos_ == \"DET\" or token.pos_ == \"AUX\" or token.pos_ == \"VERB\") for token in x.doc]) :\n",
    "        return NO\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processor : word_context_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If silence longer than 0.7s before the word -> BOS\n",
    "\n",
    "@labeling_function(pre=[word_context_markers])\n",
    "def sil_bef(x):\n",
    "    \n",
    "    if x.aft_mark!=1 and x[\"sil_bef\"]>=0.7 and x.word_bef!=\"euh\" and x.word_aft!=\"euh\" and x.word_bef!=x.word_aft and x.word_bef!=x[\"word\"] and x.word_aft!=x[\"word\"] and x[\"word\"]!=\"euh\":\n",
    "        single_markers=[\"donc\",\"effectivement\",\"alors\",\"bref\",\"voilà\", \"et\", \"ensuite\", \"mais\", \"ben\", \"hein\", \"enfin\", \"finalement\", \"pourtant\", \"cependant\"]\n",
    "        pronoms=[\"je\", \"tu\", \"il\", \"vous\", \"on\", \"nous\", \"elle\", \"ils\"]\n",
    "        \n",
    "        return BOS\n",
    "\n",
    "\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "\n",
    "# If no silence before the word -> NO\n",
    "@labeling_function(pre=[word_context_markers])\n",
    "def no_sil_bef(x):\n",
    "    beg_markers=[\"bon\",\"ouais\",\"oui\",\"pour\", \"donc\",\"effectivement\",\"alors\",\"bref\",\"voilà\", \"et\", \"ensuite\", \"mais\", \"ben\", \"hein\", \"enfin\", \"finalement\", \"pourtant\", \"cependant\", \"parce\"]\n",
    "    pronoms=[\"je\", \"tu\", \"il\", \"vous\", \"on\", \"nous\", \"elle\", \"ils\",\"qui\", \"que\", \"j'\",\"c'\"]\n",
    "    if x[\"sil_bef\"]==0 and x[\"word\"]==\"en\" and x.word_aft==\"fait\":\n",
    "            return ABSTAIN\n",
    "    elif x[\"sil_bef\"]==0 and x.marker!=1 and x[\"word\"] not in pronoms:\n",
    "        return NO\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "# If word in keyword list -> BOS\n",
    "@labeling_function(pre=[word_context_markers])\n",
    "def keywords(x):\n",
    "    liste=[\"bonjour\", \"merci\", \"ok\", \"oui\", \"non\", \"ouais\"]\n",
    "    if x.aft_mark!=1:\n",
    "        if x.word in liste or (x.word==\"d'\" and x.word_aft==\"accord\") or (x.word==\"hum\" and x.word_aft==\"hum\"):\n",
    "            return BOS \n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "# If combination of audio and spacy info are successful in dev set\n",
    "@labeling_function(pre=[word_context_markers])\n",
    "def audio_spacy(x):\n",
    "    \n",
    "   \n",
    "    dict_PC_PCe=dict(NOUN_PRON=[\"DU\", \"DS\", \"US\"],\n",
    "                    NOUN_CCONJ=[\"DD\", \"DU\", \"DS\", \"UD\", \"UU\", \"US\", \"SD\", \"SU\"],\n",
    "                    NOUN_ADV=[\"DD\"],\n",
    "                    ADJ_PRON=[\"DU\", \"DS\"],\n",
    "                    ADJ_CCONJ=[\"DD\", \"DU\", \"DS\", \"UU\"],\n",
    "                    NOUN_SCONJ=[\"DD\", \"DS\"],\n",
    "                    PROPN_PRON=[\"DU\", \"DS\", \"UD\", \"UU\"],\n",
    "                    ADV_CCONJ=[\"DD\", \"DU\", \"UD\"],\n",
    "                    VERB_CCONJ=[\"DD\"]\n",
    "                    )\n",
    "\n",
    "\n",
    "    dict_PC_PCp=dict(NOUN_PRON=[\"US\", \"DK\", \"UD\", \"SD\", \"UK\"],\n",
    "                    NOUN_CCONJ=[\"SS\",\"DD\", \"SK\",\"UD\", \"SD\", \"KS\", \"UK\"],\n",
    "                    NOUN_ADV=[\"UD\", \"KS\", \"US\", \"UU\"],\n",
    "                    ADJ_PRON=[\"SK\", \"DK\",\"UD\",\"UK\"],\n",
    "                    ADJ_CCONJ=[\"SS\",\"DD\", \"US\", \"DK\", \"UD\", \"SD\"],\n",
    "                    NOUN_SCONJ=[\"DD\",\"SK\", \"UK\"],\n",
    "                    ADJ_ADV=[\"US\",\"UD\"],\n",
    "                    PROPN_PRON=[\"SS\"],\n",
    "                    ADJ_SCONJ=[\"SS\",\"DD\", \"DK\", \"UD\"]\n",
    "                    )\n",
    "\n",
    "\n",
    "    dict_PC_PCpe=dict(NOUN_PRON=[\"DK_DS\", \"SK_DS\", \"US_DU\", \"UK_DS\", \"UD_DU\", \"SD_DU\", \"UK_DU\", \"DK_DU\"],\n",
    "                    NOUN_CCONJ=[\"SS_DU\", \"UD_DU\", \"SK_DU\"],\n",
    "                    NOUN_ADV=[\"SS_DD\", \"UD_DD\", \"US_DD\"]\n",
    "                     )\n",
    "    \n",
    "                      \n",
    "    PCpos=x.token_bef+\"_\"+x.token\n",
    "    PCe=str(x.energy_bef)+str(x.energy)\n",
    "    PCp=str(x.pitch_bef)+str(x.pitch)\n",
    "    PCp_PCe=str(x.pitch_bef)+str(x.pitch)+\"_\"+str(x.energy_bef)+str(x.energy)\n",
    "                      \n",
    "    if x.aft_mark!=1:\n",
    "        if PCpos in dict_PC_PCe.keys() and PCe in dict_PC_PCe[PCpos]:\n",
    "            return BOS\n",
    "        if PCpos in dict_PC_PCp.keys() and PCp in dict_PC_PCp[PCpos]:\n",
    "            return BOS\n",
    "        if PCpos in dict_PC_PCpe.keys() and PCp_PCe in dict_PC_PCpe[PCpos]:\n",
    "            return BOS\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "\n",
    "@labeling_function(pre=[word_context_markers])\n",
    "def spacy_pattern(x):\n",
    "    \n",
    "    beg_markers=[\"bon\",\"ouais\",\"oui\",\"pour\", \"donc\",\"effectivement\",\"alors\",\"bref\",\"voilà\", \"et\", \"ensuite\", \"mais\", \"ben\", \"hein\", \"enfin\", \"finalement\", \"pourtant\", \"cependant\", \"parce\"]\n",
    "    \n",
    "    PCpos=x.token_bef+\"_\"+x.token\n",
    "    Cdep=x.dep\n",
    "    Cheadpos=x.headpos\n",
    "    Pdep=x.dep_bef\n",
    "    Pheadpos=x.headpos_bef\n",
    "    \n",
    "    dict_PC_Cdep=dict(NOUN_PRON=[\"nsubj\", \"obj\", \"nmod\"],\n",
    "                    NOUN_CCONJ=[\"cc\"],\n",
    "                    NOUN_ADV=[\"advmod\"],\n",
    "                    NOUN_SCONJ=[\"mark\"],\n",
    "                    ADJ_ADV=[\"advmod\"],\n",
    "                    PROPN_PRON=[\"nsubj\"],\n",
    "                    ADV_CCONJ=[\"cc\"],\n",
    "                    VERB_CCONJ=[\"cc\"]\n",
    "                    )\n",
    "\n",
    "\n",
    "    dict_PC_Cheadpos=dict(NOUN_PRON=[\"ADJ\",\"PRON\",\"AUX\"],\n",
    "                        NOUN_CCONJ=[\"VERB\",\"NOUN\", \"ADJ\", \"ADV\", \"PRON\"],\n",
    "                        ADJ_PRON=[\"VERB\"],\n",
    "                        ADJ_CCONJ=[\"VERB\",\"NOUN\", \"ADJ\", \"ADV\"],\n",
    "                        NOUN_SCONJ=[\"VERB\"],\n",
    "                        PROPN_PRON=[\"VERB\",\"NOUN\"],\n",
    "                        ADV_CCONJ=[\"VERB\",\"NOUN\"],\n",
    "                        VERB_CCONJ=[\"VERB\",\"NOUN\", \"ADV\"]\n",
    "                        )\n",
    "\n",
    "\n",
    "\n",
    "    dict_PC_Pheadpos=dict(NOUN_PRON=[\"VERB\", \"ADJ\", \"PRON\"],\n",
    "                        NOUN_CCONJ=[\"VERB\", \"NOUN\", \"ADJ\", \"ADV\"],\n",
    "                        ADJ_PRON=[\"NOUN\"],\n",
    "                        ADJ_CCONJ=[\"VERB\", \"NOUN\", \"ADJ\", \"ADV\"],\n",
    "                        NOUN_SCONJ=[\"NOUN\"],\n",
    "                        ADJ_ADV=[\"NOUN\"],\n",
    "                        PROPN_PRON=[\"VERB\", \"NOUN\"],\n",
    "                        ADV_CCONJ=[\"VERB\", \"NOUN\"],\n",
    "                        VERB_CCONJ=[\"VERB\"]\n",
    "                        )\n",
    "\n",
    "    dict_PC_Pdep=dict(NOUN_PRON=[\"nomd\", \"obj\", \"obl:arg\", \"dep\", \"ccomp\"],\n",
    "                    NOUN_CCONJ=[\"nmod\", \"obj\", \"amod\", \"obj:mod\", \"obl:arg\"],\n",
    "                    NOUN_ADV=[\"nmod\"],\n",
    "                    ADJ_PRON=[\"amod\", \"xcomp\"],\n",
    "                    VERB_ADV=[\"nmod\", \"obj\", \"dep\"],\n",
    "                    NOUN_SCONJ=[\"nmod\"],\n",
    "                    ADV_CCONJ=[\"advmod\"], \n",
    "                    VERB_CCONJ=[\"xcomp\"]\n",
    "                        )\n",
    "    if x.aft_mark!=1:\n",
    "        if PCpos in dict_PC_Cdep.keys() and Cdep in dict_PC_Cdep[PCpos]:\n",
    "            return BOS\n",
    "        if PCpos in dict_PC_Pdep.keys() and Pdep in dict_PC_Pdep[PCpos]:\n",
    "            return BOS\n",
    "        if PCpos in dict_PC_Pheadpos.keys() and Pheadpos in dict_PC_Pheadpos[PCpos]:\n",
    "            return BOS\n",
    "        if PCpos in dict_PC_Cheadpos.keys() and Cheadpos in dict_PC_Cheadpos[PCpos]:\n",
    "            return BOS\n",
    "    return ABSTAIN\n",
    "\n",
    "@labeling_function(pre=[word_context_markers])\n",
    "def audio_spacy_spacy(x):\n",
    "    beg_markers=[\"bon\",\"ouais\",\"oui\",\"pour\", \"donc\",\"effectivement\",\"alors\",\"bref\",\"voilà\", \"et\", \"ensuite\", \"mais\", \"ben\", \"hein\", \"enfin\", \"finalement\", \"pourtant\", \"cependant\", \"parce\"]\n",
    "    \n",
    "    dict_PC_PCe=dict(NOUN_PRON=[\"DU\", \"DS\", \"US\"],\n",
    "                    NOUN_CCONJ=[\"DD\", \"DU\", \"DS\", \"UD\", \"UU\", \"US\", \"SD\", \"SU\"],\n",
    "                    NOUN_ADV=[\"DD\"],\n",
    "                    ADJ_PRON=[\"DU\", \"DS\"],\n",
    "                    ADJ_CCONJ=[\"DD\", \"DU\", \"DS\", \"UU\"],\n",
    "                    NOUN_SCONJ=[\"DD\", \"DS\"],\n",
    "                    PROPN_PRON=[\"DU\", \"DS\", \"UD\", \"UU\"],\n",
    "                    ADV_CCONJ=[\"DD\", \"DU\", \"UD\"],\n",
    "                    VERB_CCONJ=[\"DD\"]\n",
    "                    )\n",
    "\n",
    "\n",
    "    dict_PC_PCp=dict(NOUN_PRON=[\"US\", \"DK\", \"UD\", \"SD\", \"UK\"],\n",
    "                    NOUN_CCONJ=[\"SS\",\"DD\", \"SK\",\"UD\", \"SD\", \"KS\", \"UK\"],\n",
    "                    NOUN_ADV=[\"UD\", \"KS\", \"US\", \"UU\"],\n",
    "                    ADJ_PRON=[\"SK\", \"DK\",\"UD\",\"UK\"],\n",
    "                    ADJ_CCONJ=[\"SS\",\"DD\", \"US\", \"DK\", \"UD\", \"SD\"],\n",
    "                    NOUN_SCONJ=[\"DD\",\"SK\", \"UK\"],\n",
    "                    ADJ_ADV=[\"US\",\"UD\"],\n",
    "                    PROPN_PRON=[\"SS\"],\n",
    "                    ADJ_SCONJ=[\"SS\",\"DD\", \"DK\", \"UD\"]\n",
    "                    )\n",
    "\n",
    "\n",
    "    dict_PC_PCpe=dict(NOUN_PRON=[\"DK_DS\", \"SK_DS\", \"US_DU\", \"UK_DS\", \"UD_DU\", \"SD_DU\", \"UK_DU\", \"DK_DU\"],\n",
    "                    NOUN_CCONJ=[\"SS_DU\", \"UD_DU\", \"SK_DU\"],\n",
    "                    NOUN_ADV=[\"SS_DD\", \"UD_DD\", \"US_DD\"]\n",
    "                     )\n",
    "        \n",
    "\n",
    "    PCpos=x.token_bef+\"_\"+x.token\n",
    "    Cdep=x.dep\n",
    "    Cheadpos=x.headpos\n",
    "    Pdep=x.dep_bef\n",
    "    Pheadpos=x.headpos_bef\n",
    "    \n",
    "    dict_PC_Cdep=dict(NOUN_PRON=[\"nsubj\", \"obj\", \"nmod\"],\n",
    "                    NOUN_CCONJ=[\"cc\"],\n",
    "                    NOUN_ADV=[\"advmod\"],\n",
    "                    NOUN_SCONJ=[\"mark\"],\n",
    "                    ADJ_ADV=[\"advmod\"],\n",
    "                    PROPN_PRON=[\"nsubj\"],\n",
    "                    ADV_CCONJ=[\"cc\"],\n",
    "                    VERB_CCONJ=[\"cc\"]\n",
    "                    )\n",
    "\n",
    "\n",
    "    dict_PC_Cheadpos=dict(NOUN_PRON=[\"ADJ\",\"PRON\",\"AUX\"],\n",
    "                        NOUN_CCONJ=[\"VERB\",\"NOUN\", \"ADJ\", \"ADV\", \"PRON\"],\n",
    "                        ADJ_PRON=[\"VERB\"],\n",
    "                        ADJ_CCONJ=[\"VERB\",\"NOUN\", \"ADJ\", \"ADV\"],\n",
    "                        NOUN_SCONJ=[\"VERB\"],\n",
    "                        PROPN_PRON=[\"VERB\",\"NOUN\"],\n",
    "                        ADV_CCONJ=[\"VERB\",\"NOUN\"],\n",
    "                        VERB_CCONJ=[\"VERB\",\"NOUN\", \"ADV\"]\n",
    "                        )\n",
    "\n",
    "\n",
    "\n",
    "    dict_PC_Pheadpos=dict(NOUN_PRON=[\"VERB\", \"ADJ\", \"PRON\"],\n",
    "                        NOUN_CCONJ=[\"VERB\", \"NOUN\", \"ADJ\", \"ADV\"],\n",
    "                        ADJ_PRON=[\"NOUN\"],\n",
    "                        ADJ_CCONJ=[\"VERB\", \"NOUN\", \"ADJ\", \"ADV\"],\n",
    "                        NOUN_SCONJ=[\"NOUN\"],\n",
    "                        ADJ_ADV=[\"NOUN\"],\n",
    "                        PROPN_PRON=[\"VERB\", \"NOUN\"],\n",
    "                        ADV_CCONJ=[\"VERB\", \"NOUN\"],\n",
    "                        VERB_CCONJ=[\"VERB\"]\n",
    "                        )\n",
    "\n",
    "    dict_PC_Pdep=dict(NOUN_PRON=[\"nomd\", \"obj\", \"obl:arg\", \"dep\", \"ccomp\"],\n",
    "                    NOUN_CCONJ=[\"nmod\", \"obj\", \"amod\", \"obj:mod\", \"obl:arg\"],\n",
    "                    NOUN_ADV=[\"nmod\"],\n",
    "                    ADJ_PRON=[\"amod\", \"xcomp\"],\n",
    "                    VERB_ADV=[\"nmod\", \"obj\", \"dep\"],\n",
    "                    NOUN_SCONJ=[\"nmod\"],\n",
    "                    ADV_CCONJ=[\"advmod\"], \n",
    "                    VERB_CCONJ=[\"xcomp\"]\n",
    "                     )\n",
    "    #PCpos=x.token_bef+\"_\"+x.token\n",
    "    PCe=str(x.energy_bef)+str(x.energy)\n",
    "    PCp=str(x.pitch_bef)+str(x.pitch)\n",
    "    PCp_PCe=str(x.pitch_bef)+str(x.pitch)+\"_\"+str(x.energy_bef)+str(x.energy)\n",
    "    cmpt=0             \n",
    "    if x.aft_mark!=1:\n",
    "        if PCpos in dict_PC_PCe.keys() and PCe in dict_PC_PCe[PCpos]:\n",
    "            cmpt+=1\n",
    "        if PCpos in dict_PC_PCp.keys() and PCp in dict_PC_PCp[PCpos]:\n",
    "            cmpt+=1\n",
    "        if PCpos in dict_PC_PCpe.keys() and PCp_PCe in dict_PC_PCpe[PCpos]:\n",
    "            cmpt+=1\n",
    "        if PCpos in dict_PC_Cdep.keys() and Cdep in dict_PC_Cdep[PCpos]:\n",
    "            cmpt+=1\n",
    "        if PCpos in dict_PC_Pdep.keys() and Pdep in dict_PC_Pdep[PCpos]:\n",
    "            cmpt+=1\n",
    "        if PCpos in dict_PC_Pheadpos.keys() and Pheadpos in dict_PC_Pheadpos[PCpos]:\n",
    "            cmpt+=1\n",
    "        if PCpos in dict_PC_Cheadpos.keys() and Cheadpos in dict_PC_Cheadpos[PCpos]:\n",
    "            cmpt+=1\n",
    "    \n",
    "        if cmpt>=3:\n",
    "            return BOS\n",
    "    \n",
    "    return ABSTAIN\n",
    "    \n",
    "\n",
    "\n",
    "# If ToNy detects a Segment -> Begin of Segment (BOS)\n",
    "@labeling_function(pre=[word_context_markers])\n",
    "def tony_realturns(x):\n",
    "    beg_markers=[\"bon\",\"ouais\",\"oui\",\"pour\", \"donc\",\"effectivement\",\"alors\",\"bref\",\"voilà\", \"et\", \"ensuite\", \"mais\", \"ben\", \"hein\", \"enfin\", \"finalement\", \"pourtant\", \"cependant\", \"parce\"]\n",
    "    if x[\"tony_realturns\"]==1 and x.aft_mark!=1:\n",
    "        return BOS \n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Application of the rules on the 3 sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9752/9752 [00:55<00:00, 175.63it/s]\n",
      "100%|██████████| 8833/8833 [00:49<00:00, 176.73it/s]\n",
      "100%|██████████| 53692/53692 [04:59<00:00, 179.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "# list of rules wanted to annotate the corpus\n",
    "lfs=[ cconj, tony_realturns, first_marker,  no_type, no_sil_bef, no_after_markers, no_disfluency, period_bef, nothing_bef, keywords,no_energy_pitch, audio_spacy_spacy, audio_spacy, spacy_pattern, beg_real_turn]\n",
    "\n",
    "#lfs=[ tony_realturns, first_marker,  no_type, no_sil_bef, no_after_markers, no_disfluency, period_bef, nothing_bef, beg_real_turn]\n",
    "\n",
    "#lfs=[cconj, tony, first_marker, no_sil_bef, no_after_markers, no_disfluency, period_bef, nothing_bef, keywords, pos,cc, energy_pitch, no_energy_pitch, energy_pitch2]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_dev = applier.apply(df=df_dev)\n",
    "L_test = applier.apply(df=df_test)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelingFunction cconj, Preprocessors: [] coverage: 0.9%\n",
      "LabelingFunction tony_realturns, Preprocessors: [LambdaMapper word_context_markers, Pre: []] coverage: 10.3%\n",
      "LabelingFunction first_marker, Preprocessors: [LambdaMapper word_context_markers, Pre: []] coverage: 4.5%\n",
      "LabelingFunction no_type, Preprocessors: [SpacyPreprocessor SpacyPreprocessor, Pre: []] coverage: 34.5%\n",
      "LabelingFunction no_sil_bef, Preprocessors: [LambdaMapper word_context_markers, Pre: []] coverage: 66.5%\n",
      "LabelingFunction no_after_markers, Preprocessors: [LambdaMapper word_context_markers, Pre: []] coverage: 7.9%\n",
      "LabelingFunction no_disfluency, Preprocessors: [] coverage: 17.5%\n",
      "LabelingFunction period_bef, Preprocessors: [] coverage: 6.7%\n",
      "LabelingFunction nothing_bef, Preprocessors: [] coverage: 49.6%\n",
      "LabelingFunction keywords, Preprocessors: [LambdaMapper word_context_markers, Pre: []] coverage: 2.0%\n",
      "LabelingFunction no_energy_pitch, Preprocessors: [LambdaMapper word_context_markers, Pre: []] coverage: 40.0%\n",
      "LabelingFunction audio_spacy_spacy, Preprocessors: [LambdaMapper word_context_markers, Pre: []] coverage: 5.1%\n",
      "LabelingFunction audio_spacy, Preprocessors: [LambdaMapper word_context_markers, Pre: []] coverage: 6.4%\n",
      "LabelingFunction spacy_pattern, Preprocessors: [LambdaMapper word_context_markers, Pre: []] coverage: 8.5%\n",
      "LabelingFunction beg_real_turn, Preprocessors: [] coverage: 4.0%\n"
     ]
    }
   ],
   "source": [
    "coverage = (L_train != ABSTAIN).mean(axis=0)\n",
    "i=0\n",
    "for score in list(coverage):\n",
    "    print(lfs[i], f\"coverage: {score * 100:.1f}%\")\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_train observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cconj</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>0.003408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tony_realturns</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.103032</td>\n",
       "      <td>0.094092</td>\n",
       "      <td>0.050268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_marker</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.044588</td>\n",
       "      <td>0.038404</td>\n",
       "      <td>0.014416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_type</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.344763</td>\n",
       "      <td>0.338281</td>\n",
       "      <td>0.026671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_sil_bef</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.665071</td>\n",
       "      <td>0.619329</td>\n",
       "      <td>0.059823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_after_markers</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.079248</td>\n",
       "      <td>0.063771</td>\n",
       "      <td>0.004172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_disfluency</th>\n",
       "      <td>6</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.175147</td>\n",
       "      <td>0.161216</td>\n",
       "      <td>0.030861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period_bef</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.066565</td>\n",
       "      <td>0.063566</td>\n",
       "      <td>0.034735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothing_bef</th>\n",
       "      <td>8</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.495772</td>\n",
       "      <td>0.469753</td>\n",
       "      <td>0.011920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keywords</th>\n",
       "      <td>9</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.019407</td>\n",
       "      <td>0.016669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_energy_pitch</th>\n",
       "      <td>10</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.399817</td>\n",
       "      <td>0.387488</td>\n",
       "      <td>0.052298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audio_spacy_spacy</th>\n",
       "      <td>11</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.051106</td>\n",
       "      <td>0.051106</td>\n",
       "      <td>0.022350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audio_spacy</th>\n",
       "      <td>12</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.064218</td>\n",
       "      <td>0.063380</td>\n",
       "      <td>0.028999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spacy_pattern</th>\n",
       "      <td>13</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.085376</td>\n",
       "      <td>0.082862</td>\n",
       "      <td>0.039578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beg_real_turn</th>\n",
       "      <td>14</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.039820</td>\n",
       "      <td>0.038218</td>\n",
       "      <td>0.026410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    j Polarity  Coverage  Overlaps  Conflicts\n",
       "cconj              0   [1]      0.009424  0.009405  0.003408 \n",
       "tony_realturns     1   [1]      0.103032  0.094092  0.050268 \n",
       "first_marker       2   [1]      0.044588  0.038404  0.014416 \n",
       "no_type            3   [0]      0.344763  0.338281  0.026671 \n",
       "no_sil_bef         4   [0]      0.665071  0.619329  0.059823 \n",
       "no_after_markers   5   [0]      0.079248  0.063771  0.004172 \n",
       "no_disfluency      6   [0]      0.175147  0.161216  0.030861 \n",
       "period_bef         7   [1]      0.066565  0.063566  0.034735 \n",
       "nothing_bef        8   [0]      0.495772  0.469753  0.011920 \n",
       "keywords           9   [1]      0.019537  0.019407  0.016669 \n",
       "no_energy_pitch    10  [0]      0.399817  0.387488  0.052298 \n",
       "audio_spacy_spacy  11  [1]      0.051106  0.051106  0.022350 \n",
       "audio_spacy        12  [1]      0.064218  0.063380  0.028999 \n",
       "spacy_pattern      13  [1]      0.085376  0.082862  0.039578 \n",
       "beg_real_turn      14  [1]      0.039820  0.038218  0.026410 "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train summary \n",
    "from snorkel.labeling import LFAnalysis\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    j Polarity  Coverage  Overlaps  Conflicts\n",
      "cconj              0   [1]      0.009424  0.009405  0.003408 \n",
      "tony_realturns     1   [1]      0.103032  0.094092  0.050268 \n",
      "first_marker       2   [1]      0.044588  0.038404  0.014416 \n",
      "no_type            3   [0]      0.344763  0.338281  0.026671 \n",
      "no_sil_bef         4   [0]      0.665071  0.619329  0.059823 \n",
      "no_after_markers   5   [0]      0.079248  0.063771  0.004172 \n",
      "no_disfluency      6   [0]      0.175147  0.161216  0.030861 \n",
      "period_bef         7   [1]      0.066565  0.063566  0.034735 \n",
      "nothing_bef        8   [0]      0.495772  0.469753  0.011920 \n",
      "keywords           9   [1]      0.019537  0.019407  0.016669 \n",
      "no_energy_pitch    10  [0]      0.399817  0.387488  0.052298 \n",
      "audio_spacy_spacy  11  [1]      0.051106  0.051106  0.022350 \n",
      "audio_spacy        12  [1]      0.064218  0.063380  0.028999 \n",
      "spacy_pattern      13  [1]      0.085376  0.082862  0.039578 \n",
      "beg_real_turn      14  [1]      0.039820  0.038218  0.026410 \n"
     ]
    }
   ],
   "source": [
    "print(LFAnalysis(L=L_train, lfs=lfs).lf_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the rules on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cconj</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tony_realturns</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.098339</td>\n",
       "      <td>0.091571</td>\n",
       "      <td>0.044606</td>\n",
       "      <td>636</td>\n",
       "      <td>323</td>\n",
       "      <td>0.663191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_marker</th>\n",
       "      <td>2</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.051169</td>\n",
       "      <td>0.044504</td>\n",
       "      <td>0.016817</td>\n",
       "      <td>338</td>\n",
       "      <td>161</td>\n",
       "      <td>0.677355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_type</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.331317</td>\n",
       "      <td>0.323523</td>\n",
       "      <td>0.022559</td>\n",
       "      <td>3108</td>\n",
       "      <td>123</td>\n",
       "      <td>0.961931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_sil_bef</th>\n",
       "      <td>4</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.623564</td>\n",
       "      <td>0.582752</td>\n",
       "      <td>0.047580</td>\n",
       "      <td>5878</td>\n",
       "      <td>203</td>\n",
       "      <td>0.966617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_after_markers</th>\n",
       "      <td>5</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.088905</td>\n",
       "      <td>0.071370</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>822</td>\n",
       "      <td>45</td>\n",
       "      <td>0.948097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_disfluency</th>\n",
       "      <td>6</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.235747</td>\n",
       "      <td>0.216468</td>\n",
       "      <td>0.037531</td>\n",
       "      <td>2121</td>\n",
       "      <td>178</td>\n",
       "      <td>0.922575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period_bef</th>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.064705</td>\n",
       "      <td>0.061833</td>\n",
       "      <td>0.031993</td>\n",
       "      <td>449</td>\n",
       "      <td>182</td>\n",
       "      <td>0.711569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothing_bef</th>\n",
       "      <td>8</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.475185</td>\n",
       "      <td>0.449856</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>4618</td>\n",
       "      <td>16</td>\n",
       "      <td>0.996547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keywords</th>\n",
       "      <td>9</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.017535</td>\n",
       "      <td>0.017535</td>\n",
       "      <td>0.014869</td>\n",
       "      <td>113</td>\n",
       "      <td>58</td>\n",
       "      <td>0.660819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_energy_pitch</th>\n",
       "      <td>10</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.444832</td>\n",
       "      <td>0.427297</td>\n",
       "      <td>0.049528</td>\n",
       "      <td>4080</td>\n",
       "      <td>258</td>\n",
       "      <td>0.940526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audio_spacy_spacy</th>\n",
       "      <td>11</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.051784</td>\n",
       "      <td>0.051784</td>\n",
       "      <td>0.018765</td>\n",
       "      <td>357</td>\n",
       "      <td>148</td>\n",
       "      <td>0.706931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audio_spacy</th>\n",
       "      <td>12</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.066961</td>\n",
       "      <td>0.065422</td>\n",
       "      <td>0.025431</td>\n",
       "      <td>413</td>\n",
       "      <td>240</td>\n",
       "      <td>0.632466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spacy_pattern</th>\n",
       "      <td>13</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.083778</td>\n",
       "      <td>0.081112</td>\n",
       "      <td>0.034249</td>\n",
       "      <td>478</td>\n",
       "      <td>339</td>\n",
       "      <td>0.585067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beg_real_turn</th>\n",
       "      <td>14</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.030968</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>0.020714</td>\n",
       "      <td>298</td>\n",
       "      <td>4</td>\n",
       "      <td>0.986755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "cconj              0   [1]      0.008203  0.008203  0.002769   71        \n",
       "tony_realturns     1   [1]      0.098339  0.091571  0.044606   636       \n",
       "first_marker       2   [1]      0.051169  0.044504  0.016817   338       \n",
       "no_type            3   [0]      0.331317  0.323523  0.022559   3108      \n",
       "no_sil_bef         4   [0]      0.623564  0.582752  0.047580   5878      \n",
       "no_after_markers   5   [0]      0.088905  0.071370  0.003281   822       \n",
       "no_disfluency      6   [0]      0.235747  0.216468  0.037531   2121      \n",
       "period_bef         7   [1]      0.064705  0.061833  0.031993   449       \n",
       "nothing_bef        8   [0]      0.475185  0.449856  0.009126   4618      \n",
       "keywords           9   [1]      0.017535  0.017535  0.014869   113       \n",
       "no_energy_pitch    10  [0]      0.444832  0.427297  0.049528   4080      \n",
       "audio_spacy_spacy  11  [1]      0.051784  0.051784  0.018765   357       \n",
       "audio_spacy        12  [1]      0.066961  0.065422  0.025431   413       \n",
       "spacy_pattern      13  [1]      0.083778  0.081112  0.034249   478       \n",
       "beg_real_turn      14  [1]      0.030968  0.029840  0.020714   298       \n",
       "\n",
       "                   Incorrect  Emp. Acc.  \n",
       "cconj              9          0.887500   \n",
       "tony_realturns     323        0.663191   \n",
       "first_marker       161        0.677355   \n",
       "no_type            123        0.961931   \n",
       "no_sil_bef         203        0.966617   \n",
       "no_after_markers   45         0.948097   \n",
       "no_disfluency      178        0.922575   \n",
       "period_bef         182        0.711569   \n",
       "nothing_bef        16         0.996547   \n",
       "keywords           58         0.660819   \n",
       "no_energy_pitch    258        0.940526   \n",
       "audio_spacy_spacy  148        0.706931   \n",
       "audio_spacy        240        0.632466   \n",
       "spacy_pattern      339        0.585067   \n",
       "beg_real_turn      4          0.986755   "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev summary \n",
    "from snorkel.labeling import LFAnalysis\n",
    "a=LFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y_dev)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cconj', 'tony_realturns', 'first_marker', 'no_type', 'no_sil_bef',\n",
      "       'no_after_markers', 'no_disfluency', 'period_bef', 'nothing_bef',\n",
      "       'keywords', 'no_energy_pitch', 'audio_spacy_spacy', 'audio_spacy',\n",
      "       'spacy_pattern', 'beg_real_turn'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(a[\"Polarity\"].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOS rules score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cconj</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.062281</td>\n",
       "      <td>0.116393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tony_realturns</td>\n",
       "      <td>0.663191</td>\n",
       "      <td>0.557895</td>\n",
       "      <td>0.606003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first_marker</td>\n",
       "      <td>0.677355</td>\n",
       "      <td>0.296491</td>\n",
       "      <td>0.412447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>period_bef</td>\n",
       "      <td>0.711569</td>\n",
       "      <td>0.393860</td>\n",
       "      <td>0.507058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>keywords</td>\n",
       "      <td>0.660819</td>\n",
       "      <td>0.099123</td>\n",
       "      <td>0.172387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>audio_spacy_spacy</td>\n",
       "      <td>0.706931</td>\n",
       "      <td>0.313158</td>\n",
       "      <td>0.434043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>audio_spacy</td>\n",
       "      <td>0.632466</td>\n",
       "      <td>0.362281</td>\n",
       "      <td>0.460680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spacy_pattern</td>\n",
       "      <td>0.585067</td>\n",
       "      <td>0.419298</td>\n",
       "      <td>0.488503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>beg_real_turn</td>\n",
       "      <td>0.986755</td>\n",
       "      <td>0.261404</td>\n",
       "      <td>0.413315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                rule  precision    recall    fscore\n",
       "0  cconj              0.887500   0.062281  0.116393\n",
       "1  tony_realturns     0.663191   0.557895  0.606003\n",
       "2  first_marker       0.677355   0.296491  0.412447\n",
       "3  period_bef         0.711569   0.393860  0.507058\n",
       "4  keywords           0.660819   0.099123  0.172387\n",
       "5  audio_spacy_spacy  0.706931   0.313158  0.434043\n",
       "6  audio_spacy        0.632466   0.362281  0.460680\n",
       "7  spacy_pattern      0.585067   0.419298  0.488503\n",
       "8  beg_real_turn      0.986755   0.261404  0.413315"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1=Y_dev.sum() # N of positive points\n",
    "s0=len(Y_dev)-s1 \n",
    "\n",
    "rule=[]\n",
    "p=[]\n",
    "r=[]\n",
    "fscore=[]\n",
    "rule=[]\n",
    "\n",
    "for x in a.itertuples():\n",
    "    if x.Polarity==[1]:\n",
    "        # precision = TP / (TP + FP ) \n",
    "        precision=int(x.Correct)/(int(x.Correct)+int(x.Incorrect))\n",
    "        p.append(precision)\n",
    "        # recall = TP /(TP + FN )\n",
    "        recall=(x.Correct)/(int(x.Correct)+(int(s1-x.Correct)))\n",
    "        r.append(recall)\n",
    "        # fscore= 2.(p.r / (p+r))\n",
    "        fscore.append(2*precision*recall/(precision+recall))\n",
    "        rule.append(x.Index)\n",
    "\n",
    "df_scores=pd.DataFrame({'rule': rule, 'precision': p, 'recall': r, 'fscore': fscore})\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cconj</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.092792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tony_realturns</td>\n",
       "      <td>0.618523</td>\n",
       "      <td>0.494709</td>\n",
       "      <td>0.549731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first_marker</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.235450</td>\n",
       "      <td>0.330241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>period_bef</td>\n",
       "      <td>0.707846</td>\n",
       "      <td>0.373898</td>\n",
       "      <td>0.489325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>keywords</td>\n",
       "      <td>0.637209</td>\n",
       "      <td>0.120811</td>\n",
       "      <td>0.203113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>audio_spacy_spacy</td>\n",
       "      <td>0.612440</td>\n",
       "      <td>0.225750</td>\n",
       "      <td>0.329897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>audio_spacy</td>\n",
       "      <td>0.566421</td>\n",
       "      <td>0.270723</td>\n",
       "      <td>0.366348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spacy_pattern</td>\n",
       "      <td>0.527009</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>0.422610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>beg_real_turn</td>\n",
       "      <td>0.995146</td>\n",
       "      <td>0.361552</td>\n",
       "      <td>0.530401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                rule  precision    recall    fscore\n",
       "0  cconj              0.767123   0.049383  0.092792\n",
       "1  tony_realturns     0.618523   0.494709  0.549731\n",
       "2  first_marker       0.552795   0.235450  0.330241\n",
       "3  period_bef         0.707846   0.373898  0.489325\n",
       "4  keywords           0.637209   0.120811  0.203113\n",
       "5  audio_spacy_spacy  0.612440   0.225750  0.329897\n",
       "6  audio_spacy        0.566421   0.270723  0.366348\n",
       "7  spacy_pattern      0.527009   0.352734  0.422610\n",
       "8  beg_real_turn      0.995146   0.361552  0.530401"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "b=LFAnalysis(L=L_test, lfs=lfs).lf_summary(Y_test)\n",
    "\n",
    "s1=Y_test.sum() # N of positive points\n",
    "s0=len(Y_test)-s1 \n",
    "\n",
    "rule=[]\n",
    "p=[]\n",
    "r=[]\n",
    "fscore=[]\n",
    "rule=[]\n",
    "\n",
    "for x in b.itertuples():\n",
    "    if x.Polarity==[1]:\n",
    "        # precision = TP / (TP + FP ) \n",
    "        precision=int(x.Correct)/(int(x.Correct)+int(x.Incorrect))\n",
    "        p.append(precision)\n",
    "        # recall = TP /(TP + FN )\n",
    "        recall=(x.Correct)/(int(x.Correct)+(int(s1-x.Correct)))\n",
    "        r.append(recall)\n",
    "        # fscore= 2.(p.r / (p+r))\n",
    "        fscore.append(2*precision*recall/(precision+recall))\n",
    "        rule.append(x.Index)\n",
    "\n",
    "df_scores=pd.DataFrame({'rule': rule, 'precision': p, 'recall': r, 'fscore': fscore})\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_dev = majority_model.predict(L=L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train,Y_dev=Y_dev, n_epochs=7000, seed=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy:   88.7%\n",
      "Label Model Accuracy:     90.3%\n"
     ]
    }
   ],
   "source": [
    "majority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the generative model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label model f1 score: 0.6763485477178423\n",
      "Label model precision score: 0.6417322834645669\n",
      "Label model recall score: 0.7149122807017544\n",
      "Label model roc-auc: 0.9222876484057334\n"
     ]
    }
   ],
   "source": [
    "# dev \n",
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "preds_dev = probs_to_preds(probs_dev)\n",
    "print(\n",
    "    f\"Label model f1 score: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='f1')}\",\n",
    "    f\"Label model precision score: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='precision')}\",\n",
    "    f\"Label model recall score: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='recall')}\",\n",
    "    f\"Label model roc-auc: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='roc_auc')}\",\n",
    "    sep = \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label model f1 score: 0.6356326703343208\n",
      "Label model precision score: 0.6110659072416599\n",
      "Label model recall score: 0.6622574955908289\n",
      "Label model roc-auc: 0.9112330033012372\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "probs_test = label_model.predict_proba(L_test)\n",
    "preds_test = probs_to_preds(probs_test)\n",
    "print(\n",
    "    f\"Label model f1 score: {metric_score(Y_test, preds_test, probs=probs_test, metric='f1')}\",\n",
    "    f\"Label model precision score: {metric_score(Y_test, preds_test, probs=probs_test, metric='precision')}\",\n",
    "    f\"Label model recall score: {metric_score(Y_test, preds_test, probs=probs_test, metric='recall')}\",\n",
    "    f\"Label model roc-auc: {metric_score(Y_test, preds_test, probs=probs_test, metric='roc_auc')}\",\n",
    "    sep = \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segeval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundary Similarity =  0.5336538461538461538461538462\n",
      "Boundary Similarity tony alone =  0.4678423236514522821576763485\n",
      "Segmentation Similarity =  0.9253999179655455291222313372\n",
      "entropie croisée =  0.012618791934690074\n"
     ]
    }
   ],
   "source": [
    "import segeval as se\n",
    "\n",
    "label=[]\n",
    "for e in probs_dev:\n",
    "    if e[1]>e[0] and e[1]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "tony_list=[]       \n",
    "for x in df_dev.itertuples():\n",
    "    tony_list.append(x.tony_realturns)\n",
    "\n",
    "#print(len(Y_dev), len(label))\n",
    "gold_nltk=\"\"\n",
    "result_nltk=\"\"\n",
    "tony_nltk=\"\"\n",
    "\n",
    "\n",
    "for i in range(0,len(Y_dev)):\n",
    "    gold_nltk+=str(Y_dev[i])\n",
    "    result_nltk+=str(label[i])\n",
    "    tony_nltk+=str(tony_list[i])\n",
    "\n",
    "#print(len(gold_str), len(result_str))\n",
    "gold_masses=se.convert_nltk_to_masses(gold_nltk, boundary_symbol='1')\n",
    "result_masses=se.convert_nltk_to_masses(result_nltk, boundary_symbol='1')\n",
    "tony_masses=se.convert_nltk_to_masses(tony_nltk, boundary_symbol='1')\n",
    "nt=2\n",
    "print(\"Boundary Similarity = \", se.boundary_similarity(gold_masses, result_masses, n_t=nt))\n",
    "print(\"Boundary Similarity tony alone = \", se.boundary_similarity(gold_masses, tony_masses, n_t=nt))\n",
    "\n",
    "gold_str=se.boundary_string_from_masses(gold_masses)\n",
    "result_str=se.boundary_string_from_masses(result_masses)\n",
    "\n",
    "edit_distance=se.boundary_edit_distance(gold_str, result_str,3)\n",
    "#convert_masses_to_positions(masses)\n",
    "gold_positions=se.convert_masses_to_positions(gold_masses)\n",
    "result_positions=se.convert_masses_to_positions(result_masses)\n",
    "\n",
    "\n",
    "print(\"Segmentation Similarity = \",se.segmentation_similarity(gold_masses, result_masses))\n",
    "\n",
    "\n",
    "# Entropie croisée \n",
    "from math import *\n",
    "\n",
    "ec=0\n",
    "for i in range(0, len(probs_dev)):\n",
    "    ec+=log(probs_dev[i][1])*label[i]\n",
    "    \n",
    "ec=-ec/len(probs_dev)\n",
    "\n",
    "print(\"entropie croisée = \", ec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Label model f1 score: 0.6763485477178423\n",
      "Label model precision score: 0.6417322834645669\n",
      "Label model recall score: 0.7149122807017544\n",
      "Label model roc-auc: 0.9222876484057334\n"
     ]
    }
   ],
   "source": [
    "#calcul seuil\n",
    "print(type(preds_dev))\n",
    "\n",
    "label=[]\n",
    "for e in probs_dev:\n",
    "    if  e[1]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "preds_dev=np.array(label)\n",
    "print(\n",
    "    f\"Label model f1 score: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='f1')}\",\n",
    "    f\"Label model precision score: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='precision')}\",\n",
    "    f\"Label model recall score: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='recall')}\",\n",
    "    f\"Label model roc-auc: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric='roc_auc')}\",\n",
    "    sep = \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label distribution on the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.around(label_model.get_weights(), 2) \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_probabilities_histogram(Y):\n",
    "    plt.hist(Y, bins=10)\n",
    "    plt.xlabel(\"Probability of BOS\")\n",
    "    plt.ylabel(\"Number of data points\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "probs_train = label_model.predict_proba(L=L_train)\n",
    "plot_probabilities_histogram(probs_train[:, BOS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 0.69, 0.65, 1.  , 1.  , 0.73, 0.87, 0.61, 1.  , 0.4 , 0.98,\n",
       "       1.  , 1.  , 0.91, 0.52])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.around(label_model.get_weights(), 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of labels for data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_label_frequency(L):\n",
    "    plt.hist((L != ABSTAIN).sum(axis=1), density=True, bins=range(L.shape[1]))\n",
    "    plt.xlabel(\"Number of labels\")\n",
    "    plt.ylabel(\"Fraction of dataset\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_frequency(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Création de fichiers utiles sur la sortie du modèle génératif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Fichier pour entrainement/fine-tuning de ToNy\n",
    "<br>\n",
    "2.Fichier de visualisation des BOS de sortie du modèle génératif , comparaison avec le GOLD\n",
    "<br>\n",
    "3.Fichier de visualisation des virgules et leur lien avec les BOS\n",
    "<br>\n",
    "4.Fichier de visualisation des vrais tours VS tours détectés automatiquement \n",
    "<br>\n",
    "5.Fichier segmentation snorkel sans ponctuation\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Fichier pour entrainement/fine-tuning de ToNy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_train = label_model.predict_proba(L_train)\n",
    "\n",
    "label=[]\n",
    "for e in probs_train:\n",
    "    if e[1]>e[0] and e[1]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "\n",
    "#création fichier \n",
    "\n",
    "file_tony=open(\"file_train_tony_11032021.txt\", \"w\")\n",
    "i=0\n",
    "\n",
    "for x in df_train.itertuples():\n",
    "    if x.rank_turn==0:\n",
    "        file_tony.write(\"\\n\")\n",
    "    if label[i]==1:\n",
    "        file_tony.write(str(x.word)+\"\\t\"+\"NN\"+\"\\t\"+\"O\"+\"\\t\"+\"B-S\"+\"\\n\")\n",
    "    else:\n",
    "        file_tony.write(str(x.word)+\"\\t\"+\"NN\"+\"\\t\"+\"O\"+\"\\t\"+\"O\"+\"\\n\")\n",
    "    i+=1\n",
    "file_tony.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création fichier \n",
    "\n",
    "file_tony=open(\"file_gold_dev_tony.txt\", \"w\")\n",
    "i=0\n",
    "\n",
    "for x in df_dev.itertuples():\n",
    "    if x.rank_turn==0:\n",
    "        file_tony.write(\"\\n\")\n",
    "    if Y_dev[i]==1:\n",
    "        file_tony.write(str(x.word)+\"\\t\"+\"NN\"+\"\\t\"+\"O\"+\"\\t\"+\"B-S\"+\"\\n\")\n",
    "    else:\n",
    "        file_tony.write(str(x.word)+\"\\t\"+\"NN\"+\"\\t\"+\"O\"+\"\\t\"+\"O\"+\"\\n\")\n",
    "    i+=1\n",
    "\n",
    "file_tony.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création fichier \n",
    "\n",
    "file_tony=open(\"file_gold_test_tony.txt\", \"w\")\n",
    "i=0\n",
    "\n",
    "for x in df_test.itertuples():\n",
    "    if x.rank_turn==0:\n",
    "        file_tony.write(\"\\n\")\n",
    "    if Y_test[i]==1:\n",
    "        file_tony.write(str(x.word)+\"\\t\"+\"NN\"+\"\\t\"+\"O\"+\"\\t\"+\"B-S\"+\"\\n\")\n",
    "    else:\n",
    "        file_tony.write(str(x.word)+\"\\t\"+\"NN\"+\"\\t\"+\"O\"+\"\\t\"+\"O\"+\"\\n\")\n",
    "    i+=1\n",
    "\n",
    "file_tony.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Fichier de visualisation des BOS de sortie du modèle génératif , comparaison avec le GOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un fichier de sortie joli pour vérifier qualitativement si ça marche bien \n",
    "\n",
    "\n",
    "label=[]\n",
    "for e in probs_dev:\n",
    "    if e[1]>e[0] and e[1]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "\n",
    "#création fichier \n",
    "\n",
    "file=open(\"readable_bos_output_generative_model.txt\", \"w\")\n",
    "\n",
    "\n",
    "i=0\n",
    "liste_texte=[]\n",
    "\n",
    "for x in df_dev.itertuples():\n",
    "    mot=x.word\n",
    "    if x.real_rank_turn==0 :\n",
    "        if x.prob_period_bef<0.5 and i>0 and x.prob_comma_bef<0.5:\n",
    "            liste_texte.append(\".\")\n",
    "        liste_texte.append(\"\\n\"+\"\\n\"+\"---Spk---\"+\"\\n\"+\"\\n\")\n",
    "\n",
    "    if label[i]==1:\n",
    "        liste_texte.append(\"|D|\")\n",
    "    if Y_dev[i]==1:\n",
    "        liste_texte.append(\"|G|\")      \n",
    "    \n",
    "    \n",
    "    if x.prob_period_bef>0.5 or x.real_rank_turn==0:\n",
    "        mot=str(mot).capitalize()\n",
    "    \n",
    "    if x.prob_period>0.5:\n",
    "        mot=mot+\".\"\n",
    "\n",
    "    if x.prob_comma>0.5:\n",
    "        mot=mot+\",\"\n",
    "\n",
    "    liste_texte.append(mot)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    i+=1\n",
    "    if i==len(df_dev):\n",
    "        liste_texte.append(\".\")\n",
    "\n",
    "file.write(\" \".join(liste_texte))\n",
    "\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Fichier de visualisation des virgules et leur lien avec les BOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation des virgules \n",
    "\n",
    "label=[]\n",
    "for e in probs_dev:\n",
    "    if e[1]>e[0] and e[1]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "\n",
    "#création fichier \n",
    "\n",
    "file=open(\"virgules_bos_19012021.txt\", \"w\")\n",
    "\n",
    "\n",
    "i=0\n",
    "liste_texte=[]\n",
    "\n",
    "for x in df_dev.itertuples():\n",
    "    mot=x.word\n",
    "    if x.real_rank_turn==0 :\n",
    "        if x.prob_period_bef<0.5 and i>0 and x.prob_comma_bef<0.3:\n",
    "            liste_texte.append(\".\")\n",
    "        liste_texte.append(\"\\n\"+\"\\n\"+\"---Spk---\"+\"\\n\"+\"\\n\")\n",
    "\n",
    "    if Y_dev[i]==1:\n",
    "        liste_texte.append(\"|G|\")      \n",
    "    \n",
    "    \n",
    "    if x.prob_period_bef>0.5 or x.real_rank_turn==0:\n",
    "        mot=str(mot).capitalize()\n",
    "    \n",
    "    if x.prob_period>0.5:\n",
    "        mot=mot+\".\"\n",
    "\n",
    "    if x.prob_comma>0.3:\n",
    "        mot=mot+\",\"\n",
    "\n",
    "    liste_texte.append(mot)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    i+=1\n",
    "    if i==len(df_dev):\n",
    "        liste_texte.append(\".\")\n",
    "\n",
    "file.write(\" \".join(liste_texte))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fichier de visualisation des vrais tours VS tours détectés automatiquement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation erreur d echangmenet de speaker \n",
    "\n",
    "\n",
    "label=[]\n",
    "for e in probs_dev:\n",
    "    if e[1]>e[0] and e[1]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "\n",
    "#création fichier \n",
    "\n",
    "file=open(\"spk_change.txt\", \"w\")\n",
    "\n",
    "\n",
    "i=0\n",
    "liste_texte=[]\n",
    "\n",
    "for x in df_dev.itertuples():\n",
    "    mot=x.word\n",
    "    if x.rank_turn==0 :\n",
    "        if x.prob_period_bef<0.5 and i>0 and x.prob_comma_bef<0.5:\n",
    "            liste_texte.append(\".\")\n",
    "        liste_texte.append(\"\\n\"+\"\\n\"+\"---Spk detected---\"+\"\\n\"+\"\\n\")\n",
    "\n",
    "    if x.real_rank_turn==0:\n",
    "        liste_texte.append(\"|SPK|\")\n",
    "     \n",
    "   \n",
    "    if x.prob_period_bef>0.5 or x.real_rank_turn==0:\n",
    "        mot=str(mot).capitalize()\n",
    "    \n",
    "    if x.prob_period>0.5:\n",
    "        mot=mot+\".\"\n",
    "\n",
    "    if x.prob_comma>0.5:\n",
    "        mot=mot+\",\"\n",
    "\n",
    "    liste_texte.append(mot)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    i+=1\n",
    "    if i==len(df_dev):\n",
    "        liste_texte.append(\".\")\n",
    "\n",
    "file.write(\" \".join(liste_texte))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonjou\n"
     ]
    }
   ],
   "source": [
    "# visualisation erreur d echangmenet de speaker \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file=open(\"spk_change_comparison_R1.txt\", \"w\")\n",
    "\n",
    "\n",
    "i=0\n",
    "liste_texte=[]\n",
    "\n",
    "for x in df_dev.itertuples():\n",
    "    mot=x.word\n",
    "    if x.real_rank_turn==0 :\n",
    "        if x.prob_period_bef<0.5 and i>0 and x.prob_comma_bef<0.5:\n",
    "            liste_texte.append(\".\")\n",
    "        liste_texte.append(\"\\n\"+\"\\n\"+\"---Real turn---\"+\"\\n\"+\"\\n\")\n",
    "        \n",
    "\n",
    "    if x.rank_turn==0:\n",
    "        liste_texte.append(\"|D_SPK|\")\n",
    "    \n",
    "    #if label[i]==1:\n",
    "        #liste_texte.append(\"|D_BOS|\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    if x.prob_period_bef>0.5 or x.real_rank_turn==0:\n",
    "        mot=str(mot).capitalize()\n",
    "    \n",
    "    if x.prob_period>0.5:\n",
    "        mot=mot+\".\"\n",
    "\n",
    "    if x.prob_comma>0.5:\n",
    "        mot=mot+\",\"\n",
    "\n",
    "    liste_texte.append(mot)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    i+=1\n",
    "    if i==len(df_dev):\n",
    "        liste_texte.append(\".\")\n",
    "\n",
    "file.write(\" \".join(liste_texte))\n",
    "print(\"bonjou\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fichier segmentation snorkel sans ponctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53692 53692\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "probs_train = label_model.predict_proba(L_train)\n",
    "label=[]\n",
    "for e in probs_train:\n",
    "    if e[1]>e[0] and e[1]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "\n",
    "\n",
    "df_train_bos=df_train.copy()\n",
    "\n",
    "print(len(df_train_bos), len(label))    \n",
    "    \n",
    "df_train_bos[\"bos_snorkel\"]=label\n",
    "\n",
    "#df_train_bos.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name=['Linagora_P6', 'Linagora_C3',\n",
    "'Linagora_P5', 'Linagora_R4', 'Linagora_R3', 'Linagora_C2', 'Linagora_P4']\n",
    "for name_file in list_name:\n",
    "    file=open(\"samir/\"+name_file+\".txt\",\"w\")\n",
    "    df_r=df_train_bos[(df_train_bos[\"file\"]==name_file)]\n",
    "    #df_r.head()\n",
    "    i=0\n",
    "    text=[]\n",
    "    for x in df_r.itertuples():\n",
    "        if (x.bos_snorkel==1 or x.real_rank_turn==0) and i!=0:\n",
    "            file.write(\"\\n\"+name_file+\" \"+\" \".join(text))\n",
    "            deb=x.beg_word\n",
    "            text=[]\n",
    "\n",
    "        text.append(x.word)\n",
    "        i+=1\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8833 8833\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "probs_test = label_model.predict_proba(L_test)\n",
    "label=[]\n",
    "for e in probs_test:\n",
    "    if e[1]>e[0] and e[1]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "\n",
    "\n",
    "df_test_bos=df_test.copy()\n",
    "\n",
    "print(len(df_test_bos), len(label))    \n",
    "    \n",
    "df_test_bos[\"bos_snorkel\"]=label\n",
    "\n",
    "\n",
    "#df_train_bos.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name=['Linagora_P1', 'Linagora_C1']\n",
    "for name_file in list_name:\n",
    "    file=open(\"samir/\"+name_file+\".txt\",\"w\")\n",
    "    df_r=df_test_bos[(df_test_bos[\"file\"]==name_file)]\n",
    "\n",
    "    i=0\n",
    "    text=[]\n",
    "    for x in df_r.itertuples():\n",
    "        if (x.bos_snorkel==1 or x.real_rank_turn==0) and i!=0:\n",
    "            file.write(\"\\n\"+name_file+\" \"+\" \".join(text))\n",
    "            deb=x.beg_word\n",
    "            text=[]\n",
    "\n",
    "        text.append(x.word)\n",
    "        i+=1\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9752 9752\n"
     ]
    }
   ],
   "source": [
    "# dev\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "label=[]\n",
    "for e in probs_dev:\n",
    "    if e[1]>e[0] and e[1]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "\n",
    "\n",
    "df_dev_bos=df_dev.copy()\n",
    "\n",
    "print(len(df_dev_bos), len(label))    \n",
    "    \n",
    "df_dev_bos[\"bos_snorkel\"]=label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name=['Linagora_R1', 'Linagora_A1']\n",
    "for name_file in list_name:\n",
    "    file=open(\"samir/\"+name_file+\".txt\",\"w\")\n",
    "    df_r=df_dev_bos[(df_dev_bos[\"file\"]==name_file)]\n",
    "\n",
    "    i=0\n",
    "    text=[]\n",
    "    for x in df_r.itertuples():\n",
    "        if (x.bos_snorkel==1 or x.real_rank_turn==0) and i!=0:\n",
    "            file.write(\"\\n\"+name_file+\" \"+\" \".join(text))\n",
    "            deb=x.beg_word\n",
    "            text=[]\n",
    "\n",
    "        text.append(x.word)\n",
    "        i+=1\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dev=pd.concat([df_dev_bos, df_test_bos])\n",
    "#df_test_dev.head()\n",
    "\n",
    "df_test_dev_samir=df_test_dev[[\"word\",\"file\", \"gold\", \"bos_snorkel\"]]\n",
    "liste_name=['Linagora_R1', 'Linagora_A1','Linagora_P1', 'Linagora_C1']\n",
    "for e in liste_name:\n",
    "    df_r=df_test_dev_samir[df_test_dev_samir[\"file\"]==e]\n",
    "    df_r=df_r.rename(columns={'gold' : 'bos_gold'}) \n",
    "    df_r.to_csv(e+\".csv\", index=False)\n",
    "#df_test_dev_samir.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_samir=df_train_bos[[\"word\",\"file\",\"gold\", \"bos_snorkel\"]]\n",
    "liste_name=['Linagora_P6', 'Linagora_C3',\n",
    "'Linagora_P5', 'Linagora_R4', 'Linagora_R3', 'Linagora_C2', 'Linagora_P4']\n",
    "for e in liste_name:\n",
    "    df_r=df_train_samir[df_train_samir[\"file\"]==e]\n",
    "    df_r=df_r.rename(columns={'gold' : 'bos_gold'}) \n",
    "    df_r.to_csv(\"samir_v2/\"+e+\".csv\", index=False)\n",
    "#df_test_dev_samir.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de fichier pour la démo Gautier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53692 53692\n"
     ]
    }
   ],
   "source": [
    "# RAP 4\n",
    "probs_train = label_model.predict_proba(L_train)\n",
    "label=[]\n",
    "for e in probs_train:\n",
    "    if e[1]>e[0] and e[1]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "\n",
    "\n",
    "df_train_bos=df_train.copy()\n",
    "\n",
    "print(len(df_train_bos), len(label))    \n",
    "    \n",
    "df_train_bos[\"bos_snorkel\"]=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_bos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.74\n"
     ]
    }
   ],
   "source": [
    "#df_train_bos[\"bos_snorkel\"]=label\n",
    "\n",
    "df_train_bos.head()\n",
    "\n",
    "file=open(\"rap4_actes_dialogues.txt\",\"w\")\n",
    "\n",
    "df_r4=df_train_bos[(df_train_bos[\"file\"]==\"Linagora_R4\")]\n",
    "\n",
    "df_r4.head()\n",
    "i=0\n",
    "deb=float(df_r4.iat[0,2])\n",
    "print(deb)\n",
    "text=[]\n",
    "for x in df_r4.itertuples():\n",
    "    if x.bos_snorkel==1 and i!=0:\n",
    "        \n",
    "        file.write(\"\\n\"+\"Linagora_R4 \"+str(deb)+\" \"+str(x.beg_word)+\" \"+\" \".join(text))\n",
    "        deb=x.beg_word\n",
    "        text=[]\n",
    "        \n",
    "    text.append(x.word)\n",
    "    i+=1\n",
    "        \n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9752 9752\n"
     ]
    }
   ],
   "source": [
    "# RAP 4\n",
    "probs_dev = label_model.predict_proba(L_dev)\n",
    "label=[]\n",
    "for e in probs_dev:\n",
    "    if e[1]>e[0] and e[1]>0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "\n",
    "\n",
    "df_dev_bos=df_dev.copy()\n",
    "\n",
    "print(len(df_dev_bos), len(label))    \n",
    "    \n",
    "df_dev_bos[\"bos_snorkel\"]=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_dev_bos.head()\n",
    "\n",
    "file=open(\"rap1_actes_dialogues_VT.txt\",\"w\")\n",
    "\n",
    "df_r1=df_dev_bos[(df_dev_bos[\"file\"]==\"Linagora_R1\")]\n",
    "\n",
    "df_r1.head()\n",
    "i=0\n",
    "deb=float(df_r1.iat[0,2])\n",
    "print(deb)\n",
    "text=[]\n",
    "for x in df_r1.itertuples():\n",
    "    if (x.gold==1 or x.real_rank_turn==0) and i!=0:\n",
    "        \n",
    "        file.write(\"\\n\"+\"Linagora_R1 \"+str(deb)+\" \"+str(x.beg_word)+\" \"+\" \".join(text))\n",
    "        deb=x.beg_word\n",
    "        text=[]\n",
    "        \n",
    "    text.append(x.word)\n",
    "    i+=1\n",
    "        \n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"real_truns_timestamp\")\n",
    "\n",
    "for x in df_dev.itertuples():\n",
    "    if x.real_rank_turn==0:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linto",
   "language": "python",
   "name": "linto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
